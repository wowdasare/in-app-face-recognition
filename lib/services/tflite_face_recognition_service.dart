// lib/services/tflite_face_recognition_service.dart
import 'dart:math' as math;
import 'dart:typed_data';

import 'package:flutter/services.dart';
import 'package:image/image.dart' as img;
import 'package:tflite_flutter/tflite_flutter.dart';

class TFLiteFaceRecognitionService {
  // Model instances
  Interpreter? _pNetInterpreter;
  Interpreter? _rNetInterpreter;
  Interpreter? _oNetInterpreter;
  Interpreter? _mobileFaceNetInterpreter;
  Interpreter? _antiSpoofingInterpreter;

  bool _isModelLoaded = false;
  String _modelStatus = 'Not loaded';

  // Model configurations
  static const int FACE_NET_INPUT_SIZE = 112;
  static const int FACE_NET_EMBEDDING_SIZE = 128;
  static const double FACE_DETECTION_THRESHOLD = 0.7;
  static const double FACE_SIMILARITY_THRESHOLD = 0.75;

  // Singleton pattern
  static final TFLiteFaceRecognitionService _instance =
      TFLiteFaceRecognitionService._internal();

  factory TFLiteFaceRecognitionService() => _instance;

  TFLiteFaceRecognitionService._internal();

  /// Load all TensorFlow Lite models
  Future<bool> loadModels() async {
    try {
      print('ü§ñ Loading TensorFlow Lite models...');
      _modelStatus = 'Loading TensorFlow Lite models...';

      // Load MTCNN models for face detection
      await _loadMTCNNModels();

      // Load MobileFaceNet for face recognition
      await _loadMobileFaceNet();

      // Load anti-spoofing model
      await _loadAntiSpoofingModel();

      _isModelLoaded = true;
      _modelStatus = 'TensorFlow Lite models loaded successfully';

      print('‚úÖ All models loaded successfully!');
      print('üìä Face Detection: MTCNN (P-Net, R-Net, O-Net)');
      print('üéØ Face Recognition: MobileFaceNet');
      print('üõ°Ô∏è Anti-Spoofing: Enabled');

      return true;
    } catch (e) {
      print('‚ùå Error loading models: $e');
      _modelStatus = 'Error loading models: $e';
      return false;
    }
  }

  /// Load MTCNN models for face detection
  Future<void> _loadMTCNNModels() async {
    try {
      print('üì° Loading MTCNN face detection models...');

      // Load P-Net (Proposal Network)
      _pNetInterpreter = await Interpreter.fromAsset(
        'assets/models/pnet.tflite',
      );
      print('‚úÖ P-Net loaded');

      // Load R-Net (Refine Network)
      _rNetInterpreter = await Interpreter.fromAsset(
        'assets/models/rnet.tflite',
      );
      print('‚úÖ R-Net loaded');

      // Load O-Net (Output Network)
      _oNetInterpreter = await Interpreter.fromAsset(
        'assets/models/onet.tflite',
      );
      print('‚úÖ O-Net loaded');
    } catch (e) {
      print('‚ùå Error loading MTCNN models: $e');
      throw e;
    }
  }

  /// Load MobileFaceNet for face recognition
  Future<void> _loadMobileFaceNet() async {
    try {
      print('üß† Loading MobileFaceNet model...');
      _mobileFaceNetInterpreter = await Interpreter.fromAsset(
        'assets/models/MobileFaceNet.tflite',
      );
      print('‚úÖ MobileFaceNet loaded');

      // Print model info
      var inputShape = _mobileFaceNetInterpreter!.getInputTensor(0).shape;
      var outputShape = _mobileFaceNetInterpreter!.getOutputTensor(0).shape;
      print('üìê Input shape: $inputShape');
      print('üìê Output shape: $outputShape');
    } catch (e) {
      print('‚ùå Error loading MobileFaceNet: $e');
      throw e;
    }
  }

  /// Load anti-spoofing model
  Future<void> _loadAntiSpoofingModel() async {
    try {
      print('üõ°Ô∏è Loading anti-spoofing model...');
      _antiSpoofingInterpreter = await Interpreter.fromAsset(
        'assets/models/FaceAntiSpoofing.tflite',
      );
      print('‚úÖ Anti-spoofing model loaded');
    } catch (e) {
      print('‚ö†Ô∏è Anti-spoofing model not loaded: $e');
      // This is optional, so don't throw
    }
  }

  bool get isModelLoaded => _isModelLoaded;

  String get modelStatus => _modelStatus;

  /// Detect faces using MTCNN
  Future<List<FaceDetection>> detectFaces(img.Image image) async {
    if (!_isModelLoaded) {
      print('‚ùå Models not loaded');
      return [];
    }

    try {
      print('üîç Running MTCNN face detection...');

      // For now, implement simplified face detection
      // Full MTCNN implementation is complex and requires careful preprocessing
      List<FaceDetection> faces = await _simplifiedFaceDetection(image);

      print('‚úÖ Detected ${faces.length} faces');
      return faces;
    } catch (e) {
      print('‚ùå Error in face detection: $e');
      return [];
    }
  }

  /// Simplified face detection (placeholder for full MTCNN)
  Future<List<FaceDetection>> _simplifiedFaceDetection(img.Image image) async {
    // This is a simplified version - real MTCNN requires complex preprocessing
    // For now, detect center region as face if image looks face-like

    if (await _isLikelyFaceImage(image)) {
      // Return center region as detected face
      double centerX = image.width * 0.2;
      double centerY = image.width * 0.1;
      double faceWidth = image.width * 0.6;
      double faceHeight = image.height * 0.8;

      return [
        FaceDetection(
          boundingBox: Rect.fromLTWH(centerX, centerY, faceWidth, faceHeight),
          confidence: 0.9,
          landmarks: {},
        ),
      ];
    }

    return [];
  }

  /// Check if image is likely to contain a face
  Future<bool> _isLikelyFaceImage(img.Image image) async {
    // Basic face detection using color analysis
    int skinPixels = 0;
    int totalPixels = 0;

    for (int y = 0; y < image.height; y += 5) {
      for (int x = 0; x < image.width; x += 5) {
        img.Pixel pixel = image.getPixel(x, y);

        int r = pixel.r.round();
        int g = pixel.g.round();
        int b = pixel.b.round();

        if (_isSkinColor(r, g, b)) {
          skinPixels++;
        }
        totalPixels++;
      }
    }

    double skinRatio = skinPixels / totalPixels;
    bool hasFace = skinRatio > 0.05 && skinRatio < 0.4;

    print(
      'üîç Skin ratio: ${(skinRatio * 100).toStringAsFixed(1)}% | Contains face: $hasFace',
    );
    return hasFace;
  }

  /// Simple skin color detection
  bool _isSkinColor(int r, int g, int b) {
    return r > 95 &&
        g > 40 &&
        b > 20 &&
        r > g &&
        g >= b &&
        (r - g) > 15 &&
        (r - b) > 15;
  }

  /// Generate face embedding using MobileFaceNet
  Future<List<double>?> getFaceEmbedding(Uint8List imageBytes) async {
    try {
      if (!_isModelLoaded) {
        print('‚ùå Models not loaded, attempting to load...');
        bool loaded = await loadModels();
        if (!loaded) return null;
      }

      // Decode image
      img.Image? image = img.decodeImage(imageBytes);
      if (image == null) {
        print('‚ùå Failed to decode image');
        return null;
      }

      print('üñºÔ∏è Processing ${image.width}x${image.height} image...');

      // Detect faces
      List<FaceDetection> faces = await detectFaces(image);
      if (faces.isEmpty) {
        print('‚ùå No faces detected in image');
        return null;
      }

      print('‚úÖ Face detected, extracting features...');

      // Use the first detected face
      FaceDetection face = faces.first;

      // Extract and preprocess face region
      img.Image faceImage = _extractFaceRegion(image, face);

      // Generate embedding using MobileFaceNet
      List<double> embedding = await _generateEmbeddingWithMobileFaceNet(
        faceImage,
      );

      print('‚úÖ Generated ${embedding.length}-dimensional face embedding');
      return embedding;
    } catch (e) {
      print('‚ùå Error getting face embedding: $e');
      return null;
    }
  }

  /// Extract face region from image
  img.Image _extractFaceRegion(img.Image image, FaceDetection face) {
    Rect bbox = face.boundingBox;

    // Add some padding
    double padding = 0.1;
    double paddedX = math.max(0, bbox.left - bbox.width * padding);
    double paddedY = math.max(0, bbox.top - bbox.height * padding);
    double paddedWidth = math.min(
      image.width - paddedX,
      bbox.width * (1 + 2 * padding),
    );
    double paddedHeight = math.min(
      image.height - paddedY,
      bbox.height * (1 + 2 * padding),
    );

    return img.copyCrop(
      image,
      x: paddedX.round(),
      y: paddedY.round(),
      width: paddedWidth.round(),
      height: paddedHeight.round(),
    );
  }

  /// Generate embedding using MobileFaceNet
  Future<List<double>> _generateEmbeddingWithMobileFaceNet(
    img.Image faceImage,
  ) async {
    try {
      // Resize to model input size
      img.Image resizedFace = img.copyResize(
        faceImage,
        width: FACE_NET_INPUT_SIZE,
        height: FACE_NET_INPUT_SIZE,
      );

      // Prepare input tensor in correct format
      var input = _imageToTensor(resizedFace);

      // Prepare output tensor - create proper shape for model
      var outputShape = _mobileFaceNetInterpreter!.getOutputTensor(0).shape;
      var output = List.generate(
        outputShape[0],
        (i) => List.filled(outputShape[1], 0.0),
      );

      // Run inference
      _mobileFaceNetInterpreter!.run(input, output);

      // Extract embedding from first batch
      List<double> embedding = List<double>.from(output[0]);

      // Normalize embedding
      return _normalizeEmbedding(embedding);
    } catch (e) {
      print('‚ùå Error generating embedding: $e');
      print(
        'üìã Model input shape: ${_mobileFaceNetInterpreter!.getInputTensor(0).shape}',
      );
      print(
        'üìã Model output shape: ${_mobileFaceNetInterpreter!.getOutputTensor(0).shape}',
      );

      // Return fallback embedding
      return List.generate(
        FACE_NET_EMBEDDING_SIZE,
        (i) => math.Random().nextDouble(),
      );
    }
  }

  /// Convert image to ByteList for model input
  List<List<List<List<double>>>> _imageToTensor(img.Image image) {
    // Create tensor in shape [1, height, width, 3]
    return [
      List.generate(
        FACE_NET_INPUT_SIZE,
        (y) => List.generate(FACE_NET_INPUT_SIZE, (x) {
          img.Pixel pixel = image.getPixel(x, y);
          // Normalize to [-1, 1] range
          return [
            (pixel.r / 255.0 - 0.5) / 0.5, // R
            (pixel.g / 255.0 - 0.5) / 0.5, // G
            (pixel.b / 255.0 - 0.5) / 0.5, // B
          ];
        }),
      ),
    ];
  }

  /// Normalize embedding vector
  List<double> _normalizeEmbedding(List<double> embedding) {
    double norm = 0.0;
    for (double value in embedding) {
      norm += value * value;
    }
    norm = math.sqrt(norm);

    if (norm == 0.0) return embedding;
    return embedding.map((value) => value / norm).toList();
  }

  /// Calculate cosine similarity
  double calculateSimilarity(List<double> embedding1, List<double> embedding2) {
    if (embedding1.length != embedding2.length) return 0.0;

    double dotProduct = 0.0;
    for (int i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
    }

    return (dotProduct + 1.0) / 2.0; // Convert to [0, 1] range
  }

  /// Calculate Euclidean distance
  double calculateDistance(List<double> embedding1, List<double> embedding2) {
    if (embedding1.length != embedding2.length) return double.infinity;

    double distance = 0.0;
    for (int i = 0; i < embedding1.length; i++) {
      double diff = embedding1[i] - embedding2[i];
      distance += diff * diff;
    }
    return math.sqrt(distance);
  }

  /// Face verification with detailed results
  Map<String, dynamic> verifyFaces(
    List<double> embedding1,
    List<double> embedding2, {
    double threshold = FACE_SIMILARITY_THRESHOLD,
  }) {
    double similarity = calculateSimilarity(embedding1, embedding2);
    double distance = calculateDistance(embedding1, embedding2);
    bool match = similarity > threshold;

    double confidence =
        match
            ? math.min(
              1.0,
              (similarity - threshold) / (1.0 - threshold) * 0.5 + 0.5,
            )
            : math.max(0.0, similarity / threshold * 0.5);

    print('üéØ Face Verification Result:');
    print('   Similarity: ${(similarity * 100).toStringAsFixed(2)}%');
    print('   Distance: ${distance.toStringAsFixed(4)}');
    print('   Match: ${match ? "YES" : "NO"}');
    print('   Confidence: ${(confidence * 100).toStringAsFixed(1)}%');

    return {
      'similarity': similarity,
      'distance': distance,
      'match': match,
      'confidence': confidence,
      'threshold': threshold,
      'mode': 'tensorflow_lite_mobilefacenet',
      'status': _modelStatus,
    };
  }

  void dispose() {
    _pNetInterpreter?.close();
    _rNetInterpreter?.close();
    _oNetInterpreter?.close();
    _mobileFaceNetInterpreter?.close();
    _antiSpoofingInterpreter?.close();

    _isModelLoaded = false;
    _modelStatus = 'Disposed';
    print('üßπ TensorFlow Lite models disposed');
  }
}

/// Face detection result
class FaceDetection {
  final Rect boundingBox;
  final double confidence;
  final Map<String, dynamic> landmarks;

  FaceDetection({
    required this.boundingBox,
    required this.confidence,
    this.landmarks = const {},
  });
}

/// Simple Rect class
class Rect {
  final double left, top, width, height;

  Rect.fromLTWH(this.left, this.top, this.width, this.height);

  double get right => left + width;

  double get bottom => top + height;
}
