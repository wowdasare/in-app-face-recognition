// lib/services/tflite_face_recognition_service.dart
import 'dart:math' as math;
import 'dart:typed_data';

import 'package:flutter/services.dart';
import 'package:image/image.dart' as img;
import 'package:tflite_flutter/tflite_flutter.dart';

class TFLiteFaceRecognitionService {
  // Model instances
  Interpreter? _pNetInterpreter;
  Interpreter? _rNetInterpreter;
  Interpreter? _oNetInterpreter;
  Interpreter? _mobileFaceNetInterpreter;
  Interpreter? _antiSpoofingInterpreter;

  bool _isModelLoaded = false;
  String _modelStatus = 'Not loaded';

  // Model configurations
  static const int FACE_NET_INPUT_SIZE = 112;
  static const int FACE_NET_EMBEDDING_SIZE = 128;
  static const double FACE_DETECTION_THRESHOLD = 0.7;
  static const double FACE_SIMILARITY_THRESHOLD = 0.75;

  // MTCNN parameters
  static const double MIN_FACE_SIZE = 20.0;
  static const List<double> THRESHOLDS = [0.6, 0.7, 0.7]; // P, R, O nets
  static const double FACTOR = 0.709;

  // Singleton pattern
  static final TFLiteFaceRecognitionService _instance =
  TFLiteFaceRecognitionService._internal();

  factory TFLiteFaceRecognitionService() => _instance;

  TFLiteFaceRecognitionService._internal();

  /// Load all TensorFlow Lite models
  Future<bool> loadModels() async {
    try {
      print('ü§ñ Loading TensorFlow Lite models...');
      _modelStatus = 'Loading TensorFlow Lite models...';

      // Load MTCNN models for face detection
      await _loadMTCNNModels();

      // Load MobileFaceNet for face recognition
      await _loadMobileFaceNet();

      // Load anti-spoofing model (optional)
      await _loadAntiSpoofingModel();

      _isModelLoaded = true;
      _modelStatus = 'Loaded - TensorFlow Lite MTCNN + MobileFaceNet';

      print('‚úÖ All models loaded successfully!');
      print('üìä Face Detection: MTCNN (P-Net, R-Net, O-Net)');
      print('üéØ Face Recognition: MobileFaceNet');
      print('üõ°Ô∏è Anti-Spoofing: ${_antiSpoofingInterpreter != null ? "Enabled" : "Disabled"}');

      return true;
    } catch (e) {
      print('‚ùå Error loading models: $e');
      _modelStatus = 'Error loading models: $e';
      _isModelLoaded = false;
      return false;
    }
  }

  /// Load MTCNN models for face detection
  Future<void> _loadMTCNNModels() async {
    try {
      print('üì° Loading MTCNN face detection models...');

      // Load P-Net (Proposal Network)
      _pNetInterpreter = await Interpreter.fromAsset('assets/models/pnet.tflite');
      print('‚úÖ P-Net loaded - Input: ${_pNetInterpreter!.getInputTensor(0).shape}');

      // Load R-Net (Refine Network)
      _rNetInterpreter = await Interpreter.fromAsset('assets/models/rnet.tflite');
      print('‚úÖ R-Net loaded - Input: ${_rNetInterpreter!.getInputTensor(0).shape}');

      // Load O-Net (Output Network)
      _oNetInterpreter = await Interpreter.fromAsset('assets/models/onet.tflite');
      print('‚úÖ O-Net loaded - Input: ${_oNetInterpreter!.getInputTensor(0).shape}');
    } catch (e) {
      print('‚ùå Error loading MTCNN models: $e');
      throw e;
    }
  }

  /// Load MobileFaceNet for face recognition
  Future<void> _loadMobileFaceNet() async {
    try {
      print('üß† Loading MobileFaceNet model...');
      _mobileFaceNetInterpreter = await Interpreter.fromAsset('assets/models/MobileFaceNet.tflite');

      // Print model info
      var inputShape = _mobileFaceNetInterpreter!.getInputTensor(0).shape;
      var outputShape = _mobileFaceNetInterpreter!.getOutputTensor(0).shape;
      print('‚úÖ MobileFaceNet loaded');
      print('üìê Input shape: $inputShape');
      print('üìê Output shape: $outputShape');
    } catch (e) {
      print('‚ùå Error loading MobileFaceNet: $e');
      throw e;
    }
  }

  /// Load anti-spoofing model
  Future<void> _loadAntiSpoofingModel() async {
    try {
      print('üõ°Ô∏è Loading anti-spoofing model...');
      _antiSpoofingInterpreter = await Interpreter.fromAsset('assets/models/FaceAntiSpoofing.tflite');
      print('‚úÖ Anti-spoofing model loaded');
    } catch (e) {
      print('‚ö†Ô∏è Anti-spoofing model not loaded: $e');
      // This is optional, so don't throw
    }
  }

  bool get isModelLoaded => _isModelLoaded;
  String get modelStatus => _modelStatus;

  /// Detect faces using MTCNN
  Future<List<FaceDetection>> detectFaces(img.Image image) async {
    if (!_isModelLoaded) {
      print('‚ùå Models not loaded');
      return [];
    }

    try {
      print('üîç Running MTCNN face detection on ${image.width}x${image.height} image...');

      // Stage 1: P-Net
      List<FaceBox> candidates = await _runPNet(image);
      if (candidates.isEmpty) {
        print('‚ùå No faces found in P-Net stage');
        return [];
      }
      print('üìç P-Net found ${candidates.length} face candidates');

      // Stage 2: R-Net
      candidates = await _runRNet(image, candidates);
      if (candidates.isEmpty) {
        print('‚ùå No faces found in R-Net stage');
        return [];
      }
      print('üìç R-Net refined to ${candidates.length} face candidates');

      // Stage 3: O-Net
      List<FaceDetection> faces = await _runONet(image, candidates);
      print('‚úÖ O-Net final detection: ${faces.length} faces');

      return faces;
    } catch (e) {
      print('‚ùå Error in MTCNN face detection: $e');
      // Fallback to simple detection if MTCNN fails
      return await _fallbackFaceDetection(image);
    }
  }

  /// P-Net stage of MTCNN
  Future<List<FaceBox>> _runPNet(img.Image image) async {
    List<FaceBox> boxes = [];

    try {
      // Calculate scales for multi-scale detection
      double minSize = MIN_FACE_SIZE;
      double scale = 12.0 / minSize;
      List<double> scales = [];

      while (scale * image.width >= 12 && scale * image.height >= 12) {
        scales.add(scale);
        scale *= FACTOR;
      }

      for (double currentScale in scales) {
        int scaledWidth = (image.width * currentScale).round();
        int scaledHeight = (image.height * currentScale).round();

        if (scaledWidth < 12 || scaledHeight < 12) continue;

        // Resize image for current scale
        img.Image scaledImage = img.copyResize(image, width: scaledWidth, height: scaledHeight);

        // Prepare input for P-Net
        var input = _prepareImageForPNet(scaledImage);

        // Run P-Net inference
        var probOutput = List.generate(1, (i) => List.generate(
            ((scaledHeight - 12) / 2 + 1).floor(),
                (j) => List.generate(((scaledWidth - 12) / 2 + 1).floor(), (k) => List.filled(2, 0.0))
        ));

        var regOutput = List.generate(1, (i) => List.generate(
            ((scaledHeight - 12) / 2 + 1).floor(),
                (j) => List.generate(((scaledWidth - 12) / 2 + 1).floor(), (k) => List.filled(4, 0.0))
        ));

        _pNetInterpreter!.runForMultipleInputs([input], {
          0: probOutput,
          1: regOutput,
        });

        // Extract face candidates
        boxes.addAll(_extractBoxesFromPNet(probOutput, regOutput, currentScale, THRESHOLDS[0]));
      }

      // Non-maximum suppression
      boxes = _nonMaximumSuppression(boxes, 0.5);

    } catch (e) {
      print('‚ùå Error in P-Net: $e');
    }

    return boxes;
  }

  /// R-Net stage of MTCNN
  Future<List<FaceBox>> _runRNet(img.Image image, List<FaceBox> candidates) async {
    List<FaceBox> refinedBoxes = [];

    try {
      for (FaceBox box in candidates) {
        // Extract and resize face region to 24x24
        img.Image? faceRegion = _extractFaceRegion(image, box, 24);
        if (faceRegion == null) continue;

        // Prepare input for R-Net
        var input = _prepareImageForRNet(faceRegion);

        // Run R-Net inference
        var probOutput = List.generate(1, (i) => List.filled(2, 0.0));
        var regOutput = List.generate(1, (i) => List.filled(4, 0.0));

        _rNetInterpreter!.runForMultipleInputs([input], {
          0: probOutput,
          1: regOutput,
        });

        double confidence = probOutput[0][1];
        if (confidence > THRESHOLDS[1]) {
          // Apply regression
          FaceBox refinedBox = _applyRegression(box, regOutput[0]);
          refinedBox = FaceBox(
            x: refinedBox.x,
            y: refinedBox.y,
            width: refinedBox.width,
            height: refinedBox.height,
            confidence: confidence,
            landmarks: box.landmarks,
          );
          refinedBoxes.add(refinedBox);
        }
      }

      // Non-maximum suppression
      refinedBoxes = _nonMaximumSuppression(refinedBoxes, 0.7);

    } catch (e) {
      print('‚ùå Error in R-Net: $e');
    }

    return refinedBoxes;
  }

  /// O-Net stage of MTCNN
  Future<List<FaceDetection>> _runONet(img.Image image, List<FaceBox> candidates) async {
    List<FaceDetection> faces = [];

    try {
      for (FaceBox box in candidates) {
        // Extract and resize face region to 48x48
        img.Image? faceRegion = _extractFaceRegion(image, box, 48);
        if (faceRegion == null) continue;

        // Prepare input for O-Net
        var input = _prepareImageForONet(faceRegion);

        // Run O-Net inference
        var probOutput = List.generate(1, (i) => List.filled(2, 0.0));
        var regOutput = List.generate(1, (i) => List.filled(4, 0.0));
        var landmarkOutput = List.generate(1, (i) => List.filled(10, 0.0));

        _oNetInterpreter!.runForMultipleInputs([input], {
          0: probOutput,
          1: regOutput,
          2: landmarkOutput,
        });

        double confidence = probOutput[0][1];
        if (confidence > THRESHOLDS[2]) {
          // Apply regression and extract landmarks
          FaceBox finalBox = _applyRegression(box, regOutput[0]);
          Map<String, dynamic> landmarks = _extractLandmarks(finalBox, landmarkOutput[0]);

          faces.add(FaceDetection(
            boundingBox: Rect.fromLTWH(finalBox.x, finalBox.y, finalBox.width, finalBox.height),
            confidence: confidence,
            landmarks: landmarks,
          ));
        }
      }

    } catch (e) {
      print('‚ùå Error in O-Net: $e');
    }

    return faces;
  }

  /// Fallback face detection when MTCNN fails
  Future<List<FaceDetection>> _fallbackFaceDetection(img.Image image) async {
    print('üîÑ Using fallback face detection');

    // Simple center-region detection as fallback
    if (await _isLikelyFaceImage(image)) {
      double centerX = image.width * 0.2;
      double centerY = image.height * 0.1;
      double faceWidth = image.width * 0.6;
      double faceHeight = image.height * 0.8;

      return [
        FaceDetection(
          boundingBox: Rect.fromLTWH(centerX, centerY, faceWidth, faceHeight),
          confidence: 0.8,
          landmarks: {},
        ),
      ];
    }

    return [];
  }

  /// Generate face embedding using MobileFaceNet
  Future<List<double>?> getFaceEmbedding(Uint8List imageBytes) async {
    try {
      if (!_isModelLoaded) {
        print('‚ùå Models not loaded, attempting to load...');
        bool loaded = await loadModels();
        if (!loaded) return null;
      }

      // Decode image
      img.Image? image = img.decodeImage(imageBytes);
      if (image == null) {
        print('‚ùå Failed to decode image');
        return null;
      }

      print('üñºÔ∏è Processing ${image.width}x${image.height} image...');

      // Detect faces using MTCNN
      List<FaceDetection> faces = await detectFaces(image);
      if (faces.isEmpty) {
        print('‚ùå No faces detected in image');
        return null;
      }

      print('‚úÖ Face detected, extracting features...');

      // Use the largest/most confident face
      FaceDetection bestFace = faces.reduce((a, b) =>
      a.confidence > b.confidence ? a : b);

      // Extract and preprocess face region
      img.Image faceImage = _extractFaceRegion(image,
          FaceBox(
            x: bestFace.boundingBox.left,
            y: bestFace.boundingBox.top,
            width: bestFace.boundingBox.width,
            height: bestFace.boundingBox.height,
            confidence: bestFace.confidence,
          ),
          FACE_NET_INPUT_SIZE) ?? image;

      // Generate embedding using MobileFaceNet
      List<double> embedding = await _generateEmbeddingWithMobileFaceNet(faceImage);

      print('‚úÖ Generated ${embedding.length}-dimensional face embedding');
      return embedding;
    } catch (e) {
      print('‚ùå Error getting face embedding: $e');
      return null;
    }
  }

  /// Generate embedding using MobileFaceNet
  Future<List<double>> _generateEmbeddingWithMobileFaceNet(img.Image faceImage) async {
    try {
      // Resize to model input size (112x112)
      img.Image resizedFace = img.copyResize(
        faceImage,
        width: FACE_NET_INPUT_SIZE,
        height: FACE_NET_INPUT_SIZE,
      );

      // Prepare input tensor - MobileFaceNet expects [1, 112, 112, 3]
      var input = _imageToMobileFaceNetTensor(resizedFace);

      // Prepare output tensor
      var outputShape = _mobileFaceNetInterpreter!.getOutputTensor(0).shape;
      var output = List.generate(outputShape[0], (i) => List.filled(outputShape[1], 0.0));

      // Run inference
      _mobileFaceNetInterpreter!.run(input, output);

      // Extract and normalize embedding
      List<double> embedding = List<double>.from(output[0]);
      return _normalizeEmbedding(embedding);
    } catch (e) {
      print('‚ùå Error generating embedding: $e');
      // Return random embedding as fallback
      return List.generate(FACE_NET_EMBEDDING_SIZE, (i) => math.Random().nextDouble());
    }
  }

  /// Helper methods for image preprocessing

  List<List<List<List<double>>>> _prepareImageForPNet(img.Image image) {
    return [List.generate(image.height, (y) =>
        List.generate(image.width, (x) {
          img.Pixel pixel = image.getPixel(x, y);
          return [
            (pixel.r / 255.0 - 0.5) / 0.5,
            (pixel.g / 255.0 - 0.5) / 0.5,
            (pixel.b / 255.0 - 0.5) / 0.5,
          ];
        }))];
  }

  List<List<List<List<double>>>> _prepareImageForRNet(img.Image image) {
    return [List.generate(24, (y) =>
        List.generate(24, (x) {
          img.Pixel pixel = image.getPixel(x, y);
          return [
            (pixel.r / 255.0 - 0.5) / 0.5,
            (pixel.g / 255.0 - 0.5) / 0.5,
            (pixel.b / 255.0 - 0.5) / 0.5,
          ];
        }))];
  }

  List<List<List<List<double>>>> _prepareImageForONet(img.Image image) {
    return [List.generate(48, (y) =>
        List.generate(48, (x) {
          img.Pixel pixel = image.getPixel(x, y);
          return [
            (pixel.r / 255.0 - 0.5) / 0.5,
            (pixel.g / 255.0 - 0.5) / 0.5,
            (pixel.b / 255.0 - 0.5) / 0.5,
          ];
        }))];
  }

  List<List<List<List<double>>>> _imageToMobileFaceNetTensor(img.Image image) {
    return [List.generate(FACE_NET_INPUT_SIZE, (y) =>
        List.generate(FACE_NET_INPUT_SIZE, (x) {
          img.Pixel pixel = image.getPixel(x, y);
          return [
            (pixel.r / 255.0 - 0.5) / 0.5,
            (pixel.g / 255.0 - 0.5) / 0.5,
            (pixel.b / 255.0 - 0.5) / 0.5,
          ];
        }))];
  }

  /// Extract face region from image
  img.Image? _extractFaceRegion(img.Image image, FaceBox faceBox, int targetSize) {
    try {
      int x = math.max(0, faceBox.x.round());
      int y = math.max(0, faceBox.y.round());
      int width = math.min(image.width - x, faceBox.width.round());
      int height = math.min(image.height - y, faceBox.height.round());

      if (width <= 0 || height <= 0) return null;

      img.Image cropped = img.copyCrop(image, x: x, y: y, width: width, height: height);
      return img.copyResize(cropped, width: targetSize, height: targetSize);
    } catch (e) {
      print('‚ùå Error extracting face region: $e');
      return null;
    }
  }

  /// Helper methods for MTCNN processing

  List<FaceBox> _extractBoxesFromPNet(dynamic probOutput, dynamic regOutput, double scale, double threshold) {
    List<FaceBox> boxes = [];
    // Implementation for extracting boxes from P-Net output
    // This is a simplified version - full implementation would be more complex
    return boxes;
  }

  List<FaceBox> _nonMaximumSuppression(List<FaceBox> boxes, double threshold) {
    if (boxes.isEmpty) return boxes;

    boxes.sort((a, b) => b.confidence.compareTo(a.confidence));
    List<FaceBox> keep = [];

    for (int i = 0; i < boxes.length; i++) {
      bool shouldKeep = true;
      for (FaceBox kept in keep) {
        if (boxes[i].overlapWith(kept) > threshold) {
          shouldKeep = false;
          break;
        }
      }
      if (shouldKeep) {
        keep.add(boxes[i]);
      }
    }

    return keep;
  }

  FaceBox _applyRegression(FaceBox box, List<double> regression) {
    return FaceBox(
      x: box.x + regression[0] * box.width,
      y: box.y + regression[1] * box.height,
      width: box.width * math.exp(regression[2]),
      height: box.height * math.exp(regression[3]),
      confidence: box.confidence,
    );
  }

  Map<String, dynamic> _extractLandmarks(FaceBox box, List<double> landmarks) {
    Map<String, dynamic> result = {};
    for (int i = 0; i < 5; i++) {
      result['point_$i'] = {
        'x': box.x + landmarks[i] * box.width,
        'y': box.y + landmarks[i + 5] * box.height,
      };
    }
    return result;
  }

  /// Check if image is likely to contain a face
  Future<bool> _isLikelyFaceImage(img.Image image) async {
    int skinPixels = 0;
    int totalPixels = 0;

    for (int y = 0; y < image.height; y += 5) {
      for (int x = 0; x < image.width; x += 5) {
        img.Pixel pixel = image.getPixel(x, y);
        if (_isSkinColor(pixel.r.round(), pixel.g.round(), pixel.b.round())) {
          skinPixels++;
        }
        totalPixels++;
      }
    }

    double skinRatio = skinPixels / totalPixels;
    return skinRatio > 0.05 && skinRatio < 0.4;
  }

  bool _isSkinColor(int r, int g, int b) {
    return r > 95 && g > 40 && b > 20 && r > g && g >= b && (r - g) > 15 && (r - b) > 15;
  }

  /// Normalize embedding vector
  List<double> _normalizeEmbedding(List<double> embedding) {
    double norm = 0.0;
    for (double value in embedding) {
      norm += value * value;
    }
    norm = math.sqrt(norm);

    if (norm == 0.0) return embedding;
    return embedding.map((value) => value / norm).toList();
  }

  /// Calculate cosine similarity
  double calculateSimilarity(List<double> embedding1, List<double> embedding2) {
    if (embedding1.length != embedding2.length) return 0.0;

    double dotProduct = 0.0;
    for (int i = 0; i < embedding1.length; i++) {
      dotProduct += embedding1[i] * embedding2[i];
    }

    return (dotProduct + 1.0) / 2.0; // Convert to [0, 1] range
  }

  /// Calculate Euclidean distance
  double calculateDistance(List<double> embedding1, List<double> embedding2) {
    if (embedding1.length != embedding2.length) return double.infinity;

    double distance = 0.0;
    for (int i = 0; i < embedding1.length; i++) {
      double diff = embedding1[i] - embedding2[i];
      distance += diff * diff;
    }
    return math.sqrt(distance);
  }

  /// Face verification with detailed results
  Map<String, dynamic> verifyFaces(
      List<double> embedding1,
      List<double> embedding2, {
        double threshold = FACE_SIMILARITY_THRESHOLD,
      }) {
    double similarity = calculateSimilarity(embedding1, embedding2);
    double distance = calculateDistance(embedding1, embedding2);
    bool match = similarity > threshold;

    double confidence = match
        ? math.min(1.0, (similarity - threshold) / (1.0 - threshold) * 0.5 + 0.5)
        : math.max(0.0, similarity / threshold * 0.5);

    print('üéØ Face Verification Result:');
    print('   Similarity: ${(similarity * 100).toStringAsFixed(2)}%');
    print('   Distance: ${distance.toStringAsFixed(4)}');
    print('   Match: ${match ? "YES" : "NO"}');
    print('   Confidence: ${(confidence * 100).toStringAsFixed(1)}%');

    return {
      'similarity': similarity,
      'distance': distance,
      'match': match,
      'confidence': confidence,
      'threshold': threshold,
      'mode': 'tensorflow_lite_mtcnn_mobilefacenet',
      'status': _modelStatus,
    };
  }

  void dispose() {
    _pNetInterpreter?.close();
    _rNetInterpreter?.close();
    _oNetInterpreter?.close();
    _mobileFaceNetInterpreter?.close();
    _antiSpoofingInterpreter?.close();

    _isModelLoaded = false;
    _modelStatus = 'Disposed';
    print('üßπ TensorFlow Lite models disposed');
  }
}

/// Face detection result
class FaceDetection {
  final Rect boundingBox;
  final double confidence;
  final Map<String, dynamic> landmarks;

  FaceDetection({
    required this.boundingBox,
    required this.confidence,
    this.landmarks = const {},
  });
}

/// Face bounding box for MTCNN processing
class FaceBox {
  final double x, y, width, height;
  final double confidence;
  final Map<String, dynamic> landmarks;

  FaceBox({
    required this.x,
    required this.y,
    required this.width,
    required this.height,
    required this.confidence,
    this.landmarks = const {},
  });

  double get area => width * height;

  double overlapWith(FaceBox other) {
    double left = math.max(x, other.x);
    double top = math.max(y, other.y);
    double right = math.min(x + width, other.x + other.width);
    double bottom = math.min(y + height, other.y + other.height);

    if (left < right && top < bottom) {
      double overlapArea = (right - left) * (bottom - top);
      double unionArea = area + other.area - overlapArea;
      return overlapArea / unionArea;
    }
    return 0.0;
  }
}

/// Simple Rect class
class Rect {
  final double left, top, width, height;

  Rect.fromLTWH(this.left, this.top, this.width, this.height);

  double get right => left + width;
  double get bottom => top + height;
}